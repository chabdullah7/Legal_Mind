{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Setup"
      ],
      "metadata": {
        "id": "o3oYngny2ot_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gzEGkfdI2ZmN"
      },
      "outputs": [],
      "source": [
        "#Basic PDF & Document Libraries\n",
        "!pip install PyPDF2 pymupdf -q\n",
        "\n",
        "#Transformers, PyTorch & ML Libraries\n",
        "!pip install torch transformers sentence-transformers scikit-learn -q\n",
        "\n",
        "#LangChain & LangChain Extensions\n",
        "!pip install langchain faiss-cpu langchain-openai -q\n",
        "!pip install -U langchain-community -q"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Imports"
      ],
      "metadata": {
        "id": "AtPx-_8H2u9m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#OS & System Utilities\n",
        "import os\n",
        "from getpass import getpass\n",
        "import re\n",
        "import random\n",
        "import zipfile\n",
        "import json\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "#Google Colab / Drive\n",
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "\n",
        "\n",
        "#PDF & Document Handling\n",
        "from PyPDF2 import PdfReader\n",
        "from langchain.document_loaders import PyMuPDFLoader\n",
        "from langchain.schema import Document\n",
        "\n",
        "\n",
        "#Data Handling\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "#PyTorch & Transformers\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n",
        "\n",
        "\n",
        "#scikit-learn\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "\n",
        "#LangChain Core & Chat Models\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
        "from langchain.schema import StrOutputParser, Document\n",
        "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
        "from langchain_core.runnables import RunnableLambda, RunnableParallel, RunnablePassthrough\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.agents import initialize_agent, AgentType\n",
        "from langchain.tools import tool\n",
        "from langchain.embeddings.base import Embeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "\n",
        "#Gradio\n",
        "import gradio as gr"
      ],
      "metadata": {
        "id": "GNjBlnNP2aZ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Google Drive"
      ],
      "metadata": {
        "id": "tTf7C61K29Iy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "WYTt-oFX2acq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Law Books Splitting"
      ],
      "metadata": {
        "id": "MdOX9wF03B85"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#PDF Path\n",
        "folder_path = \"/content/drive/MyDrive/LegalMind\"\n",
        "lawbook_paths = [\n",
        "    f\"{folder_path}/Code Of Criminal Procedure 1898.pdf\",\n",
        "    f\"{folder_path}/Pakistan Penal Code.pdf\",\n",
        "    f\"{folder_path}/Qanun-e-Shahadat Order 1984.pdf\"\n",
        "]\n",
        "\n",
        "\n",
        "#Split by Numbered Sections\n",
        "def split_into_documents(text, book_name):\n",
        "    lines = text.split('\\n')\n",
        "    documents = []\n",
        "    current_chunk = \"\"\n",
        "    section_number = None\n",
        "\n",
        "    section_header_pattern = re.compile(r'^\\s*(\\d+)\\.\\s')\n",
        "\n",
        "    for line in lines:\n",
        "        match = section_header_pattern.match(line)\n",
        "        if match:\n",
        "            if current_chunk:\n",
        "                doc = Document(\n",
        "                    page_content=current_chunk.strip(),\n",
        "                    metadata={\"book\": book_name, \"section\": section_number}\n",
        "                )\n",
        "                documents.append(doc)\n",
        "                current_chunk = \"\"\n",
        "            section_number = match.group(1)\n",
        "            current_chunk = line\n",
        "        else:\n",
        "            current_chunk += '\\n' + line\n",
        "\n",
        "\n",
        "#Save last chunk\n",
        "    if current_chunk:\n",
        "        doc = Document(\n",
        "            page_content=current_chunk.strip(),\n",
        "            metadata={\"book\": book_name, \"section\": section_number}\n",
        "        )\n",
        "        documents.append(doc)\n",
        "\n",
        "    return documents\n",
        "\n",
        "\n",
        "#Process All Books\n",
        "all_chunks = []\n",
        "\n",
        "for path in lawbook_paths:\n",
        "    book_name = os.path.basename(path).replace(\".pdf\", \"\")\n",
        "    print(f\"Processing: {book_name}\")\n",
        "\n",
        "    reader = PdfReader(path)\n",
        "    text = \"\\n\".join(page.extract_text() or \"\" for page in reader.pages)\n",
        "\n",
        "    docs = split_into_documents(text, book_name)\n",
        "    all_chunks.extend(docs)\n",
        "\n",
        "    print(f\"{len(docs)} chunks created for {book_name}\\n\")\n",
        "\n",
        "#Save lawbook_chunks to a JSON file\n",
        "lawbook_raw_data = [{\"content\": doc.page_content, \"metadata\": doc.metadata} for doc in all_chunks]\n",
        "with open(\"lawbook_chunks.json\", \"w\") as f:\n",
        "    json.dump(lawbook_raw_data, f)"
      ],
      "metadata": {
        "id": "-h18CZ2y2afa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_chunks = random.sample(all_chunks, 10)\n",
        "\n",
        "for i, chunk in enumerate(sample_chunks):\n",
        "    print(f\"\\nSample Chunk #{i+1}\")\n",
        "    print(\"-\" * 50)\n",
        "    print(f\"Book: {chunk.metadata['book']}\")\n",
        "    print(f\"Section: {chunk.metadata['section']}\")\n",
        "    print(\"Content Preview:\")\n",
        "    print(chunk.page_content, \"...\\n\")"
      ],
      "metadata": {
        "id": "63aUcP2B2aiE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Judgments Splitting"
      ],
      "metadata": {
        "id": "cEdvV0sr3Jy1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = text.replace(\"_\", \"\").replace(\"-\", \"\")\n",
        "    lines = [line.strip() for line in text.splitlines() if line.strip()]\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "def is_new_judgment_page(lines):\n",
        "    if not lines:\n",
        "        return False\n",
        "\n",
        "    lines = [line.lower().strip() for line in lines[:12]]\n",
        "    joined = \" \".join(lines)\n",
        "\n",
        "    return any([\n",
        "        re.search(r\"judg[e]?ment\\s+sheet\", joined),\n",
        "        re.search(r\"peshawar high court\", joined),\n",
        "        re.search(r\"\\b(cr\\.a|crl\\.a|wp no|w\\.p\\.|writ petition|cr\\.r|cr\\.revision|c\\.p\\.|c\\.a\\.|c\\.r\\.|civil appeal|civil revision|criminal appeal|criminal misc|crl\\.misc|misc\\. appl)\\b\", joined),\n",
        "        re.match(r\"^[a-z]{2,4}\\.\\s?no\\.\\s?\\d{1,4}\", lines[0]) if lines else False\n",
        "    ])\n",
        "\n",
        "def extract_court_name(text):\n",
        "    first_3_lines = \"\\n\".join(text.strip().split(\"\\n\")[:3]).lower()\n",
        "    if \"lahore high court\" in first_3_lines or \"lahore\" in first_3_lines:\n",
        "        return \"Lahore High Court\"\n",
        "    elif \"peshawar high court\" in first_3_lines or \"peshawar\" in first_3_lines:\n",
        "        return \"Peshawar High Court\"\n",
        "    else:\n",
        "        return \"Unknown\"\n",
        "\n",
        "def split_judgments_by_common_headers(pdf_path):\n",
        "    loader = PyMuPDFLoader(pdf_path)\n",
        "    pages = loader.load()\n",
        "\n",
        "    judgments = []\n",
        "    current = []\n",
        "\n",
        "    for page in pages:\n",
        "        lines = page.page_content.strip().splitlines()\n",
        "        if is_new_judgment_page(lines) and current:\n",
        "            combined_text = \"\\n\\n\".join([p.page_content for p in current])\n",
        "            cleaned_text = clean_text(combined_text)\n",
        "            court = extract_court_name(cleaned_text)\n",
        "\n",
        "            judgments.append(Document(\n",
        "                page_content=cleaned_text,\n",
        "                metadata={\n",
        "                    \"source\": f\"Judgment {len(judgments) + 1}\",\n",
        "                    \"court\": court\n",
        "                }\n",
        "            ))\n",
        "            current = [page]\n",
        "        else:\n",
        "            current.append(page)\n",
        "\n",
        "    if current:\n",
        "        combined_text = \"\\n\\n\".join([p.page_content for p in current])\n",
        "        cleaned_text = clean_text(combined_text)\n",
        "        court = extract_court_name(cleaned_text)\n",
        "\n",
        "        judgments.append(Document(\n",
        "            page_content=cleaned_text,\n",
        "            metadata={\n",
        "                \"source\": f\"Judgment {len(judgments) + 1}\",\n",
        "                \"court\": court\n",
        "            }\n",
        "        ))\n",
        "\n",
        "    return judgments\n",
        "\n",
        "\n",
        "#Save lawbook to a JSON file\n",
        "pdf_path = \"/content/drive/MyDrive/LegalMind/Judgments.pdf\"\n",
        "\n",
        "judgments = split_judgments_by_common_headers(pdf_path)\n",
        "\n",
        "judgment_raw_data = [{\"content\": doc.page_content, \"metadata\": doc.metadata} for doc in judgments]\n",
        "with open(\"judgments_chunks.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(judgment_raw_data, f)\n"
      ],
      "metadata": {
        "id": "2NtKW3a12akd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_judgments = len(judgments)\n",
        "\n",
        "court_counts = Counter(doc.metadata.get(\"court\", \"Unknown\") for doc in judgments)\n",
        "\n",
        "print(f\"Total judgments: {total_judgments}\\n\")\n",
        "print(\"Judgments per court:\")\n",
        "for court, count in court_counts.items():\n",
        "    print(f\"{court}: {count}\")"
      ],
      "metadata": {
        "id": "VdWatpU_3NIP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, doc in enumerate(judgments[:3], 1):\n",
        "    print(f\"\\nJudgment #{i}\")\n",
        "    print(\"-\" * 50)\n",
        "    print(f\"Source: {doc.metadata.get('source', 'Unknown')}\")\n",
        "    print(f\"Court: {doc.metadata.get('court', 'Unknown')}\")\n",
        "    print(\"Full Content:\")\n",
        "    print(doc.page_content)\n",
        "    print(\"\\n\" + \"=\"*50)"
      ],
      "metadata": {
        "id": "Z_AJpQZr3NKt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##InLegalBERTEmbedder: Custom Mean-Pooling Embedder for Legal Texts"
      ],
      "metadata": {
        "id": "7Okt0W_d3VXQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class InLegalBERTEmbedder:\n",
        "    def __init__(self, model_name=\"law-ai/InLegalBERT\", device=None):\n",
        "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = AutoModel.from_pretrained(model_name).to(self.device)\n",
        "\n",
        "    def embed_documents(self, texts):\n",
        "        embeddings = []\n",
        "        for text in texts:\n",
        "            inputs = self.tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\", max_length=512).to(self.device)\n",
        "            with torch.no_grad():\n",
        "                model_output = self.model(**inputs)\n",
        "\n",
        "#Mean Pooling\n",
        "            token_embeddings = model_output.last_hidden_state\n",
        "            attention_mask = inputs[\"attention_mask\"]\n",
        "            input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "            sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
        "            sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "            embedding = sum_embeddings / sum_mask\n",
        "            embeddings.append(embedding.squeeze(0).cpu().numpy())\n",
        "        return embeddings"
      ],
      "metadata": {
        "id": "lxFaIwsr3NNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LangChain Wrapper for InLegalBERT Embeddings"
      ],
      "metadata": {
        "id": "0tHr8P0r3bAA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class InLegalBERTLangChain(Embeddings):\n",
        "    def __init__(self):\n",
        "        self.model = InLegalBERTEmbedder()\n",
        "\n",
        "    def embed_documents(self, texts):\n",
        "        return self.model.embed_documents(texts)\n",
        "\n",
        "    def embed_query(self, text):\n",
        "        return self.model.embed_documents([text])[0]"
      ],
      "metadata": {
        "id": "g7QvaD_y3NPt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Vector Stores (FAISS)"
      ],
      "metadata": {
        "id": "-pk6zEsj3fvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Load both JSON files\n",
        "with open(\"/content/judgments_chunks.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    judgment_raw_data = json.load(f)\n",
        "\n",
        "with open(\"/content/lawbook_chunks.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    lawbook_raw_data = json.load(f)\n",
        "\n",
        "#Convert JSON data to LangChain Documents\n",
        "judgment_docs = [\n",
        "    Document(page_content=item[\"content\"], metadata=item[\"metadata\"])\n",
        "    for item in judgment_raw_data\n",
        "]\n",
        "\n",
        "lawbook_docs = [\n",
        "    Document(page_content=item[\"content\"], metadata=item[\"metadata\"])\n",
        "    for item in lawbook_raw_data\n",
        "]\n",
        "\n",
        "print(f\"Loaded {len(judgment_docs)} judgment chunks\")\n",
        "print(f\"Loaded {len(lawbook_docs)} lawbook chunks\")\n",
        "\n",
        "#Initialize embedding model\n",
        "embeddings = InLegalBERTLangChain()\n",
        "\n",
        "#Create and Save FAISS vector stores\n",
        "lawbook_store = FAISS.from_documents(lawbook_docs, embeddings)\n",
        "lawbook_store.save_local(\"/content/lawbook_store\")\n",
        "print(\"Lawbook vector store saved successfully!\")\n",
        "\n",
        "judgment_store = FAISS.from_documents(judgment_docs, embeddings)\n",
        "judgment_store.save_local(\"/content/judgment_store\")\n",
        "print(\"Judgment vector store saved successfully!\")\n",
        "\n",
        "print(\"All vector stores created and saved!\")\n"
      ],
      "metadata": {
        "id": "VbSOlXOL3NR9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Load Vector Stores"
      ],
      "metadata": {
        "id": "A0mge3gP3k-E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = InLegalBERTLangChain()\n",
        "\n",
        "lawbook_store = FAISS.load_local(\"/content/lawbook_store\", embeddings, allow_dangerous_deserialization=True)\n",
        "judgment_store = FAISS.load_local(\"/content/judgment_store\", embeddings, allow_dangerous_deserialization=True)\n",
        "\n",
        "print(\"FAISS vector stores loaded successfully!\")\n"
      ],
      "metadata": {
        "id": "pMzf175m3NUh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##OpenAI LLM Initialization"
      ],
      "metadata": {
        "id": "9i4SHj9a3pPj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API key: \")\n",
        "\n",
        "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n"
      ],
      "metadata": {
        "id": "rWL3LF0D3NW2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##RAG Law Book"
      ],
      "metadata": {
        "id": "WSnCZQy13tWM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Retrieval"
      ],
      "metadata": {
        "id": "RptbM31x3w7A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retriever_law = lawbook_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})"
      ],
      "metadata": {
        "id": "iHnm85US3NY9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever_law"
      ],
      "metadata": {
        "id": "NTS5dBBr3NbV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sections = retriever_law.invoke('Explain the offence defined in Section 375 PPC and its punishment.')\n",
        "sections"
      ],
      "metadata": {
        "id": "90p3ogu93Nd7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sections[0].page_content"
      ],
      "metadata": {
        "id": "se9w1Edi3NgV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Augmentation"
      ],
      "metadata": {
        "id": "EnbxaJ1a37WH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_law = PromptTemplate(\n",
        "    input_variables=[\"context\", \"query\"],\n",
        "    template=\"\"\"\n",
        "You are a highly competent legal assistant specializing in Pakistani criminal law.\n",
        "\n",
        "Use the following sections from Pakistan law books to answer the legal query.\n",
        "\n",
        "For each section, write a separate explanation — even if the section is only partially relevant. Use this format:\n",
        "\n",
        "---\n",
        "\n",
        "Section [Section Number] – [Title or Short Description]\n",
        "From: [Book Name]\n",
        "\n",
        "[Explain how this section is relevant to the query.]\n",
        "\n",
        "---\n",
        "\n",
        "If a section is completely irrelevant, ignore it entirely and do not mention it in the answer.\n",
        "\n",
        "Respond in formal legal language only.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Query:\n",
        "{query}\n",
        "\"\"\"\n",
        ")"
      ],
      "metadata": {
        "id": "i01R1GV13Ni2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is the procedure for issuing a warrant of arrest in Pakistan?\"\n",
        "retrieved_sections = retriever_law.invoke(query)\n",
        "retrieved_sections"
      ],
      "metadata": {
        "id": "OXbt6Thz3NlV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not isinstance(retrieved_sections, list):\n",
        "    retrieved_sections = [retrieved_sections]\n",
        "\n",
        "context_text = \"\\n\\n---\\n\\n\".join([\n",
        "    f\"Section from {doc.metadata.get('book', 'Unknown Book')} (Section {doc.metadata.get('section', 'N/A')}):\\n{doc.page_content}\"\n",
        "    for doc in retrieved_sections\n",
        "])"
      ],
      "metadata": {
        "id": "ZaKw08-Z3Nnu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_prompt_law = prompt_law.invoke({\"context\": context_text, \"query\": query})\n",
        "final_prompt_law"
      ],
      "metadata": {
        "id": "hd_WNUbl2am1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Generation"
      ],
      "metadata": {
        "id": "TGgji9Mm4Hct"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "answer = llm.invoke(final_prompt_law)\n",
        "print(answer.content)"
      ],
      "metadata": {
        "id": "6gSV-BWO4Em0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Chain"
      ],
      "metadata": {
        "id": "7ZZr5_Nf4MbT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def format_docs(docs):\n",
        "    return \"\\n\\n---\\n\\n\".join([\n",
        "        f\"Section {doc.metadata.get('section', 'N/A')} – {doc.metadata.get('title', 'No Title')}\\n\\n\"\n",
        "        f\"From: {doc.metadata.get('book', 'Unknown Book')}\\n\\n\"\n",
        "        f\"{doc.page_content.strip()}\"\n",
        "        for doc in docs\n",
        "    ])\n",
        "\n",
        "parallel_chain_law = RunnableParallel({\n",
        "    \"context\": retriever_law | RunnableLambda(format_docs),\n",
        "    \"query\": RunnablePassthrough()\n",
        "})\n",
        "\n",
        "parser = StrOutputParser()\n",
        "\n",
        "main_chain_law = parallel_chain_law | prompt_law | llm | parser"
      ],
      "metadata": {
        "id": "dZ7DX4GM4Epm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = main_chain_law.invoke(\"What are the legal procedures for search and seizure during a criminal investigation under Pakistani law?\")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "-92AwIt54Er8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##RAG Judgment"
      ],
      "metadata": {
        "id": "zuSLLwuj4TBO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Retriever"
      ],
      "metadata": {
        "id": "jCHtTL4U4Zv-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retriever_judgment = judgment_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
        "retriever_judgment"
      ],
      "metadata": {
        "id": "AXqX0CsY4EuE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_judgment = retriever_judgment.invoke('Explain the offence defined in Section 375 PPC and its punishment.')\n",
        "prompt_judgment"
      ],
      "metadata": {
        "id": "DZnSHG0M4EwX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_judgment[0].page_content"
      ],
      "metadata": {
        "id": "63AnmCJV4Eyj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Augmentation"
      ],
      "metadata": {
        "id": "oP5wEPkK4142"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_judgment = ChatPromptTemplate.from_template(\"\"\"\n",
        "You are a legal assistant trained in Pakistani law. Given the following legal judgment text, extract and summarize the key legal information in a structured manner.\n",
        "\n",
        "Format the output like this:\n",
        "\n",
        "Case Type: [e.g. Writ Petition, Criminal Appeal]\n",
        "Court: [e.g. Lahore High Court, Supreme Court of Pakistan]\n",
        "Parties Involved:\n",
        "- Petitioner(s): [Names if available]\n",
        "- Respondent(s): [Names if available]\n",
        "\n",
        "Main Legal Issues:\n",
        "[Summarize the core legal issues raised in the case.]\n",
        "\n",
        "Petitioner’s Arguments:\n",
        "[Summarize what the petitioner argued.]\n",
        "\n",
        "Respondent’s Arguments:\n",
        "[Summarize what the respondent argued.]\n",
        "\n",
        "Relevant Laws and Sections:\n",
        "[List all mentioned laws/sections (e.g., Section 154 CrPC, Article 10A of the Constitution).]\n",
        "\n",
        "Court’s Reasoning and Observations:\n",
        "[Summarize how the court analyzed the matter, referring to any case law, principles, or interpretations.]\n",
        "\n",
        "Final Decision / Order:\n",
        "[Summarize what the court ordered — dismissed, allowed, directions issued, etc.]\n",
        "\n",
        "---\n",
        "Summary of the Judgment:\n",
        "[Provide a concise summary of the entire judgment in no more than 10 lines.]\n",
        "\n",
        "Judgment Text:\n",
        "{context}\n",
        "\"\"\")\n"
      ],
      "metadata": {
        "id": "eUwbLciw4E1J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Summarize the judgment regarding registration of FIR under section 154 CrPC\"\n",
        "retrieved_judgments = retriever_judgment.invoke(query)\n",
        "retrieved_judgments"
      ],
      "metadata": {
        "id": "bSbQbGrP4E3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Chain"
      ],
      "metadata": {
        "id": "I-uXtRJ748d5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "judgment_summary_chain = RunnableLambda(\n",
        "    lambda query: [\n",
        "        (prompt_judgment | llm | StrOutputParser()).invoke({\"context\": doc.page_content})\n",
        "        for doc in retriever_judgment.invoke(query)\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "m2M2TMqo4E5u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"criteria for pre-arrest bail in Pakistan\"\n",
        "\n",
        "response = judgment_summary_chain.invoke(query)\n",
        "\n",
        "print_output = \"\\n\\n-------\\n\\n\".join(response)\n",
        "print(print_output)"
      ],
      "metadata": {
        "id": "BhG8ly6L4E-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Legal Case Verdict Prediction Pipeline"
      ],
      "metadata": {
        "id": "-GdoylgD5E3K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"/content/drive/MyDrive/Legal Mind/best_model\"\n",
        "\n",
        "#Load model & tokenizer\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_path, local_files_only=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)\n",
        "model.eval()\n",
        "print(\"Model & tokenizer loaded successfully.\")\n",
        "\n",
        "\n",
        "with open(\"/content/drive/MyDrive/LegalMind/LegalMind.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    df = json.load(f)\n",
        "df = pd.DataFrame(df)\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "label_encoder.fit(df[\"verdict\"])\n",
        "label_mapping = dict(zip(label_encoder.transform(label_encoder.classes_), label_encoder.classes_))\n",
        "\n",
        "\n",
        "def clean_text(text):\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(r\"[\\n\\r\\t]\", \" \", text)\n",
        "    text = re.sub(r\"[\\\"']\", \"\", text)\n",
        "    text = re.sub(r\"[^a-z0-9 ,.\\[\\]()/\\-:]\", \"\", text)\n",
        "    text = re.sub(r\"\\.{2,}\", \".\", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    text = re.sub(r\"[,/]+\", \",\", text)\n",
        "    text = re.sub(r\"(,\\s*,)+\", \",\", text)\n",
        "    text = re.sub(r\"(,\\s*$)|(^\\s*,)\", \"\", text)\n",
        "    text = re.sub(r\"\\b(p\\s*,\\s*,\\s*,\\s*c)\\b\", \"ppc\", text)\n",
        "    return text.strip()\n",
        "\n",
        "\n",
        "def format_case(case):\n",
        "    summary = clean_text(case[\"summary\"])\n",
        "    pet = clean_text(case[\"petitioner_argument\"])\n",
        "    resp = clean_text(case[\"respondent_argument\"])\n",
        "    case_type = clean_text(case.get(\"case_type\", \"\"))\n",
        "    sections = clean_text(\", \".join(case.get(\"offence_sections\", [])))\n",
        "    return f\"[SUMMARY] {summary} [PETITIONER] {pet} [RESPONDENT] {resp} [CASE TYPE] {case_type} [SECTIONS] {sections}\"\n",
        "\n",
        "\n",
        "def predict_legal_verdict(case: dict) -> str:\n",
        "    formatted_text = format_case(case)\n",
        "    inputs = tokenizer(formatted_text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        predicted_label = torch.argmax(outputs.logits, dim=1).item()\n",
        "    return label_mapping[predicted_label]\n",
        "\n",
        "\n",
        "llm = ChatOpenAI(temperature=0)\n",
        "\n",
        "prompt_extract_case = PromptTemplate.from_template(\"\"\"\n",
        "Extract the following fields from the legal case text:\n",
        "- summary\n",
        "- petitioner_argument\n",
        "- respondent_argument\n",
        "- offence_sections (list like [\"489-F\", \"420\"])\n",
        "- case_type\n",
        "\n",
        "Respond in JSON format.\n",
        "\n",
        "Case Text:\n",
        "{text}\n",
        "\"\"\")\n",
        "\n",
        "output_parser = JsonOutputParser()\n",
        "extract_case_chain = prompt_extract_case | llm | output_parser\n",
        "\n",
        "def run_pipeline(raw_text: str):\n",
        "    print(\"Extracting structured case data from LLM...\")\n",
        "    structured_case = extract_case_chain.invoke({\"text\": raw_text})\n",
        "    print(\"Structured fields:\", json.dumps(structured_case, indent=2))\n",
        "\n",
        "    print(\"Predicting verdict using LegalBERT classifier...\")\n",
        "    verdict = predict_legal_verdict(structured_case)\n",
        "    print(\"Predicted Verdict:\", verdict)\n",
        "    return verdict\n"
      ],
      "metadata": {
        "id": "hM0ZKB_A5A7L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Tools"
      ],
      "metadata": {
        "id": "jVdBA7G75KO9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@tool\n",
        "def lawbook_tool(query: str) -> str:\n",
        "    \"\"\"\n",
        "    Use this tool when the user is asking about specific Pakistani laws, legal provisions, or statutory sections.\n",
        "    It retrieves and summarizes relevant sections from digitized Pakistani law books (e.g., PPC, CrPC, PECA).\n",
        "\n",
        "    Example queries:\n",
        "    - \"Explain Section 489-F about cheque dishonor.\"\n",
        "    - \"What does Section 302 of the Pakistan Penal Code say?\"\n",
        "    - \"Give me details of cybercrime law under PECA.\"\n",
        "    - \"Is there any section in CrPC related to bail?\"\n",
        "\n",
        "    The tool returns a summarized explanation of the most relevant sections from law books.\n",
        "    \"\"\"\n",
        "    k = 3\n",
        "\n",
        "    #Retrieve relevant law book content\n",
        "    retrieved = retriever_law.invoke(query)\n",
        "\n",
        "    if not retrieved:\n",
        "        return \"No relevant law sections found.\"\n",
        "\n",
        "    section_titles = {}\n",
        "\n",
        "    #Top-k results\n",
        "    top_docs = retrieved[:k]\n",
        "\n",
        "    #Format context with metadata\n",
        "    formatted_context = \"\\n\\n---\\n\\n\".join([\n",
        "        (\n",
        "            f\"Section {doc.metadata.get('section', 'N/A')}\"\n",
        "            + (\n",
        "                f\" – {doc.metadata.get('title')}\"\n",
        "                if doc.metadata.get('title')\n",
        "                else (\n",
        "                    f\" – {section_titles.get(doc.metadata.get('section', ''), '')}\"\n",
        "                    if doc.metadata.get('section', '') in section_titles else \"\"\n",
        "                )\n",
        "            )\n",
        "            + f\"\\n\\nFrom: {doc.metadata.get('book', 'Unknown Book')}\\n\\n\"\n",
        "            + f\"{doc.page_content.strip()}\"\n",
        "        )\n",
        "        for doc in top_docs\n",
        "    ])\n",
        "\n",
        "    #Summarize using LLM\n",
        "    input_data = {\"context\": formatted_context, \"query\": query}\n",
        "    response = (prompt_law | llm | StrOutputParser()).invoke(input_data)\n",
        "\n",
        "    return response\n",
        "\n",
        "#Judment\n",
        "@tool\n",
        "\n",
        "def judgment_tool(query: str) -> str:\n",
        "    \"\"\"\n",
        "    Use this tool when the user asks about how Pakistani courts have decided similar cases in the past.\n",
        "    It retrieves and summarizes legal judgments, precedents, or case law from a database of actual court decisions.\n",
        "\n",
        "    Example queries:\n",
        "    - \"What did the court decide in cheque dishonor cases?\"\n",
        "    - \"Any Supreme Court ruling on Section 489-F?\"\n",
        "    - \"Has Section 302 ever been challenged in court?\"\n",
        "\n",
        "    The tool returns a summary of top relevant legal judgments (default: 3) based on the user's query.\n",
        "    \"\"\"\n",
        "    k = 3\n",
        "\n",
        "    #Retrieve relevant court judgments\n",
        "    retrieved = retriever_judgment.invoke(query)\n",
        "\n",
        "    if not retrieved:\n",
        "        return \"No relevant judgments found.\"\n",
        "\n",
        "    #Top-k documents only\n",
        "    top_docs = retrieved[:k]\n",
        "\n",
        "    #Summarize\n",
        "    results = []\n",
        "    for doc in top_docs:\n",
        "        context = doc.page_content\n",
        "        input_data = {\"context\": context}\n",
        "        response = (prompt_judgment | llm | StrOutputParser()).invoke(input_data)\n",
        "        results.append(response)\n",
        "\n",
        "    return \"\\n\\n-------\\n\\n\".join(results)\n",
        "\n",
        "\n",
        "#Predictor\n",
        "@tool\n",
        "def predict_verdict_from_text(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Use this tool when the user provides raw legal case text (such as FIRs, arguments, case summaries, or judgment excerpts)\n",
        "    and wants to predict what the verdict might be based on the structured information in that text.\n",
        "\n",
        "    The tool extracts structured fields such as:\n",
        "    - Petitioner Argument\n",
        "    - Respondent Argument\n",
        "    - Offence Sections\n",
        "    - Case Type\n",
        "    - Case Summary\n",
        "\n",
        "    Then it uses a trained LegalBERT-based model to predict the likely court verdict.\n",
        "\n",
        "    Example queries:\n",
        "    - \"Predict the outcome of this case: [paste full case text]\"\n",
        "    - \"Based on this FIR and arguments, what might the court decide?\"\n",
        "    - \"Here's a summary and sections applied. Predict the result.\"\n",
        "\n",
        "    The tool returns the predicted verdict such as: 'Guilty', 'Not Guilty', 'Dismissed', 'Granted', etc., depending on the case type.\n",
        "    \"\"\"\n",
        "    structured_case = extract_case_chain.invoke({\"text\": text})\n",
        "    verdict = predict_legal_verdict(structured_case)\n",
        "    return verdict"
      ],
      "metadata": {
        "id": "mIMinuNq5A91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Agent"
      ],
      "metadata": {
        "id": "Ilxc5zgU5Oo0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tools = [\n",
        "    judgment_tool,\n",
        "    lawbook_tool,\n",
        "    predict_verdict_from_text\n",
        "]\n",
        "\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "\n",
        "\n",
        "\n",
        "agent = initialize_agent(\n",
        "    tools=tools,\n",
        "    llm=llm,\n",
        "    agent=AgentType.OPENAI_FUNCTIONS,\n",
        "    verbose=True,\n",
        "    memory = memory\n",
        ")"
      ],
      "metadata": {
        "id": "9ELKrzEe5BAL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = agent.invoke(\"What are the legal consequences of cyber harassment under Pakistani law?\")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "nssTmIQJ5BCq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Gradio"
      ],
      "metadata": {
        "id": "hfx_t5Ff5Vs9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(temperature=0)\n",
        "\n",
        "\n",
        "@tool\n",
        "def lawbook_tool(query: str) -> str:\n",
        "    \"\"\"Use this tool for legal provisions or sections (e.g., 489-F, PECA, PPC).\"\"\"\n",
        "    return get_lawbook_sections(query)\n",
        "\n",
        "@tool\n",
        "def judgment_tool(query: str) -> str:\n",
        "    \"\"\"Use this tool to fetch past case judgments relevant to a legal issue.\"\"\"\n",
        "    return get_past_judgments(query)\n",
        "\n",
        "@tool\n",
        "def predict_verdict_from_text(raw_text: str) -> str:\n",
        "    \"\"\"Use this tool when a full legal case is given for verdict prediction.\"\"\"\n",
        "    return predict_verdict(raw_text)\n",
        "\n",
        "tools = [judgment_tool, lawbook_tool, predict_verdict_from_text]\n",
        "\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "\n",
        "agent = initialize_agent(\n",
        "    tools=tools,\n",
        "    llm=llm,\n",
        "    agent=AgentType.OPENAI_FUNCTIONS,\n",
        "    verbose=True,\n",
        "    memory=memory\n",
        ")\n",
        "\n",
        "\n",
        "#Functional Blocks\n",
        "def predict_verdict(raw_text):\n",
        "    structured_case = extract_case_chain.invoke({\"text\": raw_text})\n",
        "    verdict = predict_legal_verdict(structured_case)\n",
        "    return verdict\n",
        "\n",
        "def get_lawbook_sections(query):\n",
        "    retrieved = retriever_law.invoke(query)\n",
        "    if not retrieved:\n",
        "        return \"No relevant sections found.\"\n",
        "\n",
        "    top_docs = retrieved[:3]\n",
        "    results = []\n",
        "    for doc in top_docs:\n",
        "        context = (\n",
        "            f\"Section {doc.metadata.get('section', 'N/A')} – {doc.metadata.get('title', 'No Title')}\\n\\n\"\n",
        "            f\"From: {doc.metadata.get('book', 'Unknown Book')}\\n\\n\"\n",
        "            f\"{doc.page_content.strip()}\"\n",
        "        )\n",
        "        prompt_input = {\"context\": context, \"query\": query}\n",
        "        response = (prompt_law | llm | StrOutputParser()).invoke(prompt_input)\n",
        "        results.append(response)\n",
        "\n",
        "    return \"\\n\\n---\\n\\n\".join(results)\n",
        "\n",
        "def get_past_judgments(query):\n",
        "    retrieved = retriever_judgment.invoke(query)\n",
        "    if not retrieved:\n",
        "        return \"No relevant judgments found.\"\n",
        "\n",
        "    top_docs = retrieved[:3]\n",
        "    results = []\n",
        "    for doc in top_docs:\n",
        "        context = doc.page_content\n",
        "        response = (prompt_judgment | llm | StrOutputParser()).invoke({\"context\": context})\n",
        "        results.append(response)\n",
        "\n",
        "    return \"\\n\\n---\\n\\n\".join(results)\n",
        "\n",
        "\n",
        "\n",
        "#Chat Function with Memory\n",
        "def chat_fn(message, chat_history):\n",
        "    try:\n",
        "        chat_history = chat_history or []\n",
        "\n",
        "#Context from chat history\n",
        "        history_text = \"\"\n",
        "        for user_msg, ai_msg in chat_history:\n",
        "            history_text += f\"User: {user_msg}\\nAI: {ai_msg}\\n\"\n",
        "\n",
        "        full_input = f\"{history_text}User: {message}\"\n",
        "\n",
        "\n",
        "#Manual routing based on keywords\n",
        "        if any(word in message.lower() for word in [\"section\", \"law\", \"pecca\", \"ppc\", \"crpc\"]):\n",
        "            reply = lawbook_tool.invoke(message)\n",
        "        else:\n",
        "            reply = agent.invoke({\"input\": full_input})[\"output\"]\n",
        "\n",
        "        chat_history.append((message, reply))\n",
        "        return chat_history, chat_history\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = f\"Error: {str(e)}\"\n",
        "        chat_history = chat_history or []\n",
        "        chat_history.append((message, error_msg))\n",
        "        return chat_history, chat_history\n",
        "\n",
        "\n",
        "#Gradio UI\n",
        "with gr.Blocks(title=\"LegalMind AI: Justice Meets Intelligence\") as demo:\n",
        "    gr.Markdown(\"LegalMind AI — Justice Meets Intelligence\")\n",
        "\n",
        "\n",
        "#Predict Verdict Tab\n",
        "    with gr.Tab(\"Predict Verdict\"):\n",
        "        input_text = gr.Textbox(lines=15, label=\"Enter Legal Case Text\")\n",
        "        verdict_output = gr.Code(label=\"Predicted Verdict\", language=\"python\")\n",
        "        verdict_btn = gr.Button(\"Predict Verdict\")\n",
        "        verdict_btn.click(predict_verdict, inputs=input_text, outputs=verdict_output)\n",
        "\n",
        "\n",
        "#Law Book Tab\n",
        "    with gr.Tab(\"Search Law Books\"):\n",
        "        law_query = gr.Textbox(label=\"Ask about a legal topic (e.g., cybercrime, 489-F, bail)\", lines=3)\n",
        "        law_output = gr.Code(label=\"Relevant Legal Sections\", language=\"python\")\n",
        "        law_btn = gr.Button(\"Search Law Books\")\n",
        "        law_btn.click(get_lawbook_sections, inputs=law_query, outputs=law_output)\n",
        "\n",
        "\n",
        "#Past Judgments Tab\n",
        "    with gr.Tab(\"Past Case Judgments\"):\n",
        "        case_query = gr.Textbox(label=\"Describe the legal issue (e.g., 'cheque dishonor under 489-F')\", lines=3)\n",
        "        judgment_output = gr.Code(label=\"Top Past Judgments\", language=\"python\")\n",
        "        judgment_btn = gr.Button(\"Find Past Judgments\")\n",
        "        judgment_btn.click(get_past_judgments, inputs=case_query, outputs=judgment_output)\n",
        "\n",
        "\n",
        "    #ChatBot Tab\n",
        "    with gr.Tab(\"Legal ChatBot\"):\n",
        "        chatbot = gr.Chatbot(label=\"LegalMind Chat\", height=400)\n",
        "        msg = gr.Textbox(label=\"Ask your legal question...\")\n",
        "        state = gr.State([])\n",
        "\n",
        "        send_btn = gr.Button(\"Send\")\n",
        "        send_btn.click(chat_fn, inputs=[msg, state], outputs=[chatbot, state])\n",
        "        msg.submit(chat_fn, inputs=[msg, state], outputs=[chatbot, state])\n",
        "\n",
        "demo.launch(share=True)"
      ],
      "metadata": {
        "id": "5VhgdkS35BE5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}